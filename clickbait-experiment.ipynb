{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting Workspace and Creating Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version:  1.7.0\n",
      "msa-practice\taustraliaeast\tmsa-practice\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)\n",
    "print(ws.name, ws.location, ws.resource_group, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "exp_name = \"clickbait\"\n",
    "\n",
    "experiment = Experiment(workspace=ws, name=exp_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset from \"clickbait_data.csv\" - contains 32000 titles of articles, each assigned a binary label indicating whether it is clickbait (0: not clickbait, 1: clickbait). Each category hsa 16000 titles. Verify that there are no missing values in dataset. Then, preprocess each line by converting each line to lowercase, strip all punctuation, and replace numbers with a identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['headline', 'clickbait'], dtype='object')\n",
      "Empty values:\n",
      "headline     0\n",
      "clickbait    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"clickbait_data.csv\")\n",
    "print(df.columns)\n",
    "df.head(10)\n",
    "\n",
    "# Check for NA values - none found\n",
    "print(\"Empty values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>clickbait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>should i get bings</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>which tv female friend group do you belong in</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the new star wars the force awakens trailer is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this vine of new york on celebrity big brother...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a couple did a stunning photo shoot with their...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>how to flirt with queer girls without making a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>num cute things to distract from your awkward ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>if disney princesses were from florida</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>whats a quote or lyric that best describes you...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>natalie dormer and sam claflin play a game to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  clickbait\n",
       "0                                 should i get bings          1\n",
       "1      which tv female friend group do you belong in          1\n",
       "2  the new star wars the force awakens trailer is...          1\n",
       "3  this vine of new york on celebrity big brother...          1\n",
       "4  a couple did a stunning photo shoot with their...          1\n",
       "5  how to flirt with queer girls without making a...          1\n",
       "6  num cute things to distract from your awkward ...          1\n",
       "7             if disney princesses were from florida          1\n",
       "8  whats a quote or lyric that best describes you...          1\n",
       "9  natalie dormer and sam claflin play a game to ...          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process(line):\n",
    "    import re, string\n",
    "    \n",
    "    #To lowercase\n",
    "    line = line.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    line = line.translate(line.maketrans('','', string.punctuation))\n",
    "    \n",
    "    # Replace digits\n",
    "    line = re.sub('\\d+', 'num', line)\n",
    "    \n",
    "    return line\n",
    "\n",
    "# Preprocess each line\n",
    "for (idx, (headline, _)) in df.iterrows():\n",
    "    newline = process(headline)\n",
    "    df.iloc[idx, 0] = newline\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset into 80% training data and 20% validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                headline\n",
      "29038                      san diego votes for new mayor\n",
      "22599  american war deserter given stay of deportatio...\n",
      "5009   emily blunt and john krasinski are officially ...\n",
      "21680  nascar driver jimmie johnson becomes first tim...\n",
      "2597   this american was shocked when his facebook po...\n",
      "28379    harper the red ensign of num will fly over vimy\n",
      "3024         what planet in the star wars galaxy is this\n",
      "31233        england win second test of the ashes series\n",
      "18957           read santa clara marks tenth anniversary\n",
      "31793          boston celtics win numth nba championship\n",
      "                                                headline\n",
      "17306        low turnout may mar congo republic election\n",
      "13994            which snl character matches your zodiac\n",
      "24615            jefferson to face forward on new nickel\n",
      "30062     number of homeowners  facing foreclosure rises\n",
      "8420   heres what happens when you mix beer and mac a...\n",
      "15650  num inspiring roald dahl quotes guaranteed to ...\n",
      "31073  atlanta thrashers sold to true north sports an...\n",
      "6071   num incredibly easy ways to get your shit toge...\n",
      "8416         what combination of hogwarts houses are you\n",
      "581            num animals so weird theyre probably fake\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_x = df.copy()\n",
    "df_y = df_x.pop('clickbait')\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.2)\n",
    "\n",
    "print(X_train[0:10])\n",
    "print(X_test[0:10])\n",
    "\n",
    "# Convert dataframe to numpy array\n",
    "X_train_original = np.squeeze(X_train.values)\n",
    "X_test_original = np.squeeze(X_test.values)\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a fully-connected 3 layer neural network with Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(top_words, title_length):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    from keras.layers.embeddings import Embedding\n",
    "    from keras.layers import Flatten\n",
    "\n",
    "    embedding_vector_length = 32\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, embedding_vector_length, input_length=title_length))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract most common words and convert titles into numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 25, 32)            3200      \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 16)                12816     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 16,305\n",
      "Trainable params: 16,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "[[1, 1, 1, 9, 22, 1], [1, 1, 1, 1, 1, 6, 1, 4, 1], [1, 1, 10, 1, 1, 14, 1, 1, 1, 1], [1, 1, 1, 1, 1, 54, 70, 1], [16, 1, 50, 1, 62, 65, 1, 1, 23, 1, 1, 1]]\n",
      "[[1, 1, 1, 1, 1, 1, 1], [29, 1, 1, 1, 12, 85], [1, 3, 1, 1, 11, 22, 1], [1, 6, 1, 1, 1, 1], [67, 24, 1, 62, 7, 1, 1, 10, 1, 10, 1]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25600 samples, validate on 6400 samples\n",
      "Epoch 1/10\n",
      "25600/25600 [==============================] - 1s 21us/step - loss: 0.3631 - accuracy: 0.8387 - val_loss: 0.2272 - val_accuracy: 0.9100\n",
      "Epoch 2/10\n",
      "25600/25600 [==============================] - 0s 15us/step - loss: 0.2072 - accuracy: 0.9155 - val_loss: 0.2043 - val_accuracy: 0.9198\n",
      "Epoch 3/10\n",
      "25600/25600 [==============================] - 0s 14us/step - loss: 0.1975 - accuracy: 0.9191 - val_loss: 0.2051 - val_accuracy: 0.9230\n",
      "Epoch 4/10\n",
      "25600/25600 [==============================] - 0s 15us/step - loss: 0.1920 - accuracy: 0.9228 - val_loss: 0.1973 - val_accuracy: 0.9236\n",
      "Epoch 5/10\n",
      "25600/25600 [==============================] - 0s 14us/step - loss: 0.1882 - accuracy: 0.9244 - val_loss: 0.1967 - val_accuracy: 0.9237\n",
      "Epoch 6/10\n",
      "25600/25600 [==============================] - 0s 14us/step - loss: 0.1872 - accuracy: 0.9247 - val_loss: 0.1942 - val_accuracy: 0.9252\n",
      "Epoch 7/10\n",
      "25600/25600 [==============================] - 0s 14us/step - loss: 0.1831 - accuracy: 0.9262 - val_loss: 0.2039 - val_accuracy: 0.9244\n",
      "Epoch 8/10\n",
      "25600/25600 [==============================] - 0s 14us/step - loss: 0.1807 - accuracy: 0.9271 - val_loss: 0.1974 - val_accuracy: 0.9253\n",
      "Epoch 9/10\n",
      "25600/25600 [==============================] - 0s 13us/step - loss: 0.1800 - accuracy: 0.9279 - val_loss: 0.1906 - val_accuracy: 0.9277\n",
      "Epoch 10/10\n",
      "25600/25600 [==============================] - 0s 14us/step - loss: 0.1759 - accuracy: 0.9300 - val_loss: 0.1916 - val_accuracy: 0.9280\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 25, 32)            6400      \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                12816     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 19,505\n",
      "Trainable params: 19,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "[[1, 1, 1, 9, 22, 1], [180, 1, 1, 1, 1, 6, 1, 4, 1], [1, 1, 10, 1, 1, 14, 1, 1, 1, 1], [1, 1, 1, 1, 1, 54, 70, 1], [16, 180, 50, 1, 62, 65, 1, 1, 23, 1, 1, 1]]\n",
      "[[1, 1, 134, 1, 1, 1, 1], [29, 1, 111, 1, 12, 85], [1, 3, 1, 1, 11, 22, 1], [1, 6, 1, 1, 1, 1], [67, 24, 1, 62, 7, 1, 1, 10, 1, 10, 1]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25600 samples, validate on 6400 samples\n",
      "Epoch 1/10\n",
      "25600/25600 [==============================] - 1s 21us/step - loss: 0.3225 - accuracy: 0.8711 - val_loss: 0.2033 - val_accuracy: 0.9258\n",
      "Epoch 2/10\n",
      "25600/25600 [==============================] - 0s 14us/step - loss: 0.1709 - accuracy: 0.9323 - val_loss: 0.1687 - val_accuracy: 0.9342\n",
      "Epoch 3/10\n",
      "25600/25600 [==============================] - 0s 14us/step - loss: 0.1587 - accuracy: 0.9364 - val_loss: 0.1635 - val_accuracy: 0.9388\n",
      "Epoch 4/10\n",
      "25600/25600 [==============================] - 0s 14us/step - loss: 0.1565 - accuracy: 0.9381 - val_loss: 0.1598 - val_accuracy: 0.9367\n",
      "Epoch 5/10\n",
      "25600/25600 [==============================] - 0s 14us/step - loss: 0.1497 - accuracy: 0.9420 - val_loss: 0.1602 - val_accuracy: 0.9389\n",
      "Epoch 6/10\n",
      "25600/25600 [==============================] - 0s 14us/step - loss: 0.1469 - accuracy: 0.9420 - val_loss: 0.1585 - val_accuracy: 0.9406\n",
      "Epoch 7/10\n",
      "25600/25600 [==============================] - 0s 14us/step - loss: 0.1419 - accuracy: 0.9442 - val_loss: 0.1583 - val_accuracy: 0.9406\n",
      "Epoch 8/10\n",
      "25600/25600 [==============================] - 0s 13us/step - loss: 0.1398 - accuracy: 0.9446 - val_loss: 0.1626 - val_accuracy: 0.9375\n",
      "Epoch 9/10\n",
      "25600/25600 [==============================] - 0s 14us/step - loss: 0.1350 - accuracy: 0.9461 - val_loss: 0.1581 - val_accuracy: 0.9422\n",
      "Epoch 10/10\n",
      "25600/25600 [==============================] - 0s 13us/step - loss: 0.1323 - accuracy: 0.9481 - val_loss: 0.1597 - val_accuracy: 0.9413\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 25, 32)            16000     \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 16)                12816     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 29,105\n",
      "Trainable params: 29,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "[[1, 1, 1, 9, 22, 1], [180, 285, 1, 1, 1, 6, 1, 4, 302], [1, 1, 10, 391, 1, 14, 1, 1, 451, 296], [1, 1, 1, 1, 1, 54, 70, 1], [16, 180, 50, 1, 62, 65, 456, 1, 23, 252, 337, 1]]\n",
      "[[1, 1, 134, 1, 1, 1, 230], [29, 1, 111, 1, 12, 85], [1, 3, 329, 1, 11, 22, 1], [1, 6, 1, 1, 1, 1], [67, 24, 1, 62, 7, 1, 1, 10, 1, 10, 1]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25600 samples, validate on 6400 samples\n",
      "Epoch 1/10\n",
      "25600/25600 [==============================] - 1s 21us/step - loss: 0.3305 - accuracy: 0.8425 - val_loss: 0.1445 - val_accuracy: 0.9448\n",
      "Epoch 2/10\n",
      "25600/25600 [==============================] - 0s 15us/step - loss: 0.1279 - accuracy: 0.9508 - val_loss: 0.1260 - val_accuracy: 0.9514\n",
      "Epoch 3/10\n",
      "25600/25600 [==============================] - 0s 14us/step - loss: 0.1132 - accuracy: 0.9572 - val_loss: 0.1220 - val_accuracy: 0.9539\n",
      "Epoch 4/10\n",
      "25600/25600 [==============================] - 0s 14us/step - loss: 0.1038 - accuracy: 0.9602 - val_loss: 0.1306 - val_accuracy: 0.9498\n",
      "Epoch 5/10\n",
      "25600/25600 [==============================] - 0s 14us/step - loss: 0.0986 - accuracy: 0.9616 - val_loss: 0.1236 - val_accuracy: 0.9528\n",
      "Epoch 6/10\n",
      "25600/25600 [==============================] - 0s 14us/step - loss: 0.0915 - accuracy: 0.9652 - val_loss: 0.1251 - val_accuracy: 0.9536\n",
      "Epoch 7/10\n",
      "25600/25600 [==============================] - 0s 13us/step - loss: 0.0858 - accuracy: 0.9662 - val_loss: 0.1295 - val_accuracy: 0.9531\n",
      "Epoch 8/10\n",
      "25600/25600 [==============================] - 0s 14us/step - loss: 0.0820 - accuracy: 0.9685 - val_loss: 0.1443 - val_accuracy: 0.9467\n",
      "Epoch 9/10\n",
      "25600/25600 [==============================] - 0s 14us/step - loss: 0.0765 - accuracy: 0.9711 - val_loss: 0.1355 - val_accuracy: 0.9531\n",
      "Epoch 10/10\n",
      "25600/25600 [==============================] - 0s 13us/step - loss: 0.0719 - accuracy: 0.9730 - val_loss: 0.1441 - val_accuracy: 0.9516\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 25, 32)            32000     \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 16)                12816     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 45,105\n",
      "Trainable params: 45,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "[[582, 1, 1, 9, 22, 1], [180, 285, 1, 1, 932, 6, 1, 4, 302], [1, 1, 10, 391, 1, 14, 1, 634, 451, 296], [1, 1, 1, 1, 596, 54, 70, 1], [16, 180, 50, 1, 62, 65, 456, 711, 23, 252, 337, 1]]\n",
      "[[1, 1, 134, 1, 1, 1, 230], [29, 1, 111, 613, 12, 85], [1, 3, 329, 1, 11, 22, 1], [1, 6, 1, 1, 1, 1], [67, 24, 637, 62, 7, 1, 1, 10, 1, 10, 761]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25600 samples, validate on 6400 samples\n",
      "Epoch 1/10\n",
      "25600/25600 [==============================] - 1s 21us/step - loss: 0.2913 - accuracy: 0.8796 - val_loss: 0.1158 - val_accuracy: 0.9616\n",
      "Epoch 2/10\n",
      "25600/25600 [==============================] - 0s 15us/step - loss: 0.0972 - accuracy: 0.9639 - val_loss: 0.0975 - val_accuracy: 0.9661\n",
      "Epoch 3/10\n",
      "25600/25600 [==============================] - 0s 14us/step - loss: 0.0780 - accuracy: 0.9712 - val_loss: 0.1096 - val_accuracy: 0.9600\n",
      "Epoch 4/10\n",
      "25600/25600 [==============================] - 0s 14us/step - loss: 0.0667 - accuracy: 0.9759 - val_loss: 0.0970 - val_accuracy: 0.9653\n",
      "Epoch 5/10\n",
      "25600/25600 [==============================] - 0s 14us/step - loss: 0.0581 - accuracy: 0.9788 - val_loss: 0.1020 - val_accuracy: 0.9652\n",
      "Epoch 6/10\n",
      "25600/25600 [==============================] - 0s 14us/step - loss: 0.0483 - accuracy: 0.9826 - val_loss: 0.1087 - val_accuracy: 0.9639\n",
      "Epoch 7/10\n",
      "25600/25600 [==============================] - 0s 14us/step - loss: 0.0415 - accuracy: 0.9846 - val_loss: 0.1250 - val_accuracy: 0.9589\n",
      "Epoch 8/10\n",
      "25600/25600 [==============================] - 0s 14us/step - loss: 0.0364 - accuracy: 0.9873 - val_loss: 0.1277 - val_accuracy: 0.9619\n",
      "Epoch 9/10\n",
      "25600/25600 [==============================] - 0s 14us/step - loss: 0.0309 - accuracy: 0.9893 - val_loss: 0.1412 - val_accuracy: 0.9577\n",
      "Epoch 10/10\n",
      "25600/25600 [==============================] - 0s 15us/step - loss: 0.0277 - accuracy: 0.9906 - val_loss: 0.1471 - val_accuracy: 0.9586\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 25, 32)            64000     \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 16)                12816     \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 77,105\n",
      "Trainable params: 77,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "[[582, 1, 1033, 9, 22, 1189], [180, 285, 1, 1324, 932, 6, 1, 4, 302], [1, 1, 10, 391, 1, 14, 1461, 634, 451, 296], [1525, 1064, 1, 1240, 596, 54, 70, 1], [16, 180, 50, 1, 62, 65, 456, 711, 23, 252, 337, 1588]]\n",
      "[[1261, 1, 134, 1, 1835, 1572, 230], [29, 1, 111, 613, 12, 85], [1, 3, 329, 1, 11, 22, 1], [1578, 6, 1, 1512, 1, 1447], [67, 24, 637, 62, 7, 1, 1507, 10, 1, 10, 761]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25600 samples, validate on 6400 samples\n",
      "Epoch 1/10\n",
      "25600/25600 [==============================] - 1s 23us/step - loss: 0.2923 - accuracy: 0.8776 - val_loss: 0.1063 - val_accuracy: 0.9631\n",
      "Epoch 2/10\n",
      "25600/25600 [==============================] - 0s 15us/step - loss: 0.0754 - accuracy: 0.9729 - val_loss: 0.0799 - val_accuracy: 0.9717\n",
      "Epoch 3/10\n",
      "25600/25600 [==============================] - 0s 16us/step - loss: 0.0555 - accuracy: 0.9804 - val_loss: 0.0809 - val_accuracy: 0.9712\n",
      "Epoch 4/10\n",
      "25600/25600 [==============================] - 0s 16us/step - loss: 0.0422 - accuracy: 0.9850 - val_loss: 0.0858 - val_accuracy: 0.9702\n",
      "Epoch 5/10\n",
      "25600/25600 [==============================] - 0s 16us/step - loss: 0.0323 - accuracy: 0.9893 - val_loss: 0.0923 - val_accuracy: 0.9678\n",
      "Epoch 6/10\n",
      "25600/25600 [==============================] - 0s 17us/step - loss: 0.0243 - accuracy: 0.9913 - val_loss: 0.1053 - val_accuracy: 0.9655\n",
      "Epoch 7/10\n",
      "25600/25600 [==============================] - 0s 16us/step - loss: 0.0177 - accuracy: 0.9944 - val_loss: 0.1090 - val_accuracy: 0.9669\n",
      "Epoch 8/10\n",
      "25600/25600 [==============================] - 0s 16us/step - loss: 0.0131 - accuracy: 0.9961 - val_loss: 0.1225 - val_accuracy: 0.9644\n",
      "Epoch 9/10\n",
      "25600/25600 [==============================] - 0s 15us/step - loss: 0.0111 - accuracy: 0.9968 - val_loss: 0.1311 - val_accuracy: 0.9645\n",
      "Epoch 10/10\n",
      "25600/25600 [==============================] - 0s 15us/step - loss: 0.0084 - accuracy: 0.9978 - val_loss: 0.1357 - val_accuracy: 0.9636\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 25, 32)            160000    \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 16)                12816     \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 173,105\n",
      "Trainable params: 173,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "[[582, 2530, 1033, 9, 22, 1189], [180, 285, 1, 1324, 932, 6, 4796, 4, 302], [3145, 4797, 10, 391, 4233, 14, 1461, 634, 451, 296], [1525, 1064, 1, 1240, 596, 54, 70, 2109], [16, 180, 50, 3756, 62, 65, 456, 711, 23, 252, 337, 1588]]\n",
      "[[1261, 1, 134, 1, 1835, 1572, 230], [29, 2266, 111, 613, 12, 85], [1, 3, 329, 4270, 11, 22, 1], [1578, 6, 1, 1512, 1, 1447], [67, 24, 637, 62, 7, 2785, 1507, 10, 2061, 10, 761]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25600 samples, validate on 6400 samples\n",
      "Epoch 1/10\n",
      "25600/25600 [==============================] - 1s 26us/step - loss: 0.2821 - accuracy: 0.8879 - val_loss: 0.0803 - val_accuracy: 0.9706\n",
      "Epoch 2/10\n",
      "25600/25600 [==============================] - 0s 19us/step - loss: 0.0532 - accuracy: 0.9816 - val_loss: 0.0690 - val_accuracy: 0.9753\n",
      "Epoch 3/10\n",
      "25600/25600 [==============================] - 0s 19us/step - loss: 0.0267 - accuracy: 0.9914 - val_loss: 0.0698 - val_accuracy: 0.9742\n",
      "Epoch 4/10\n",
      "25600/25600 [==============================] - 0s 18us/step - loss: 0.0130 - accuracy: 0.9966 - val_loss: 0.0820 - val_accuracy: 0.9734\n",
      "Epoch 5/10\n",
      "25600/25600 [==============================] - 0s 18us/step - loss: 0.0060 - accuracy: 0.9987 - val_loss: 0.0914 - val_accuracy: 0.9708\n",
      "Epoch 6/10\n",
      "25600/25600 [==============================] - 0s 18us/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.0990 - val_accuracy: 0.9706\n",
      "Epoch 7/10\n",
      "25600/25600 [==============================] - 0s 18us/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.1111 - val_accuracy: 0.9706\n",
      "Epoch 8/10\n",
      "25600/25600 [==============================] - 0s 18us/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.1191 - val_accuracy: 0.9697\n",
      "Epoch 9/10\n",
      "25600/25600 [==============================] - 0s 19us/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.1182 - val_accuracy: 0.9703\n",
      "Epoch 10/10\n",
      "25600/25600 [==============================] - 0s 18us/step - loss: 7.7850e-04 - accuracy: 0.9998 - val_loss: 0.1272 - val_accuracy: 0.9691\n"
     ]
    }
   ],
   "source": [
    "# Extract most common words\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.preprocessing import sequence\n",
    "#from tensorflow.keras.models import load_model\n",
    "\n",
    "top_words_array = [100, 200, 500, 1000, 2000, 5000]\n",
    "\n",
    "for top_words in top_words_array:\n",
    "    run = experiment.start_logging()\n",
    "    run.log(\"top_word_count\", top_words)\n",
    "    \n",
    "    X_train = X_train_original.copy()\n",
    "    X_test = X_test_original.copy()\n",
    "    \n",
    "    title_length = 25\n",
    "    \n",
    "    model = create_model(top_words, title_length)\n",
    "    tokenizer = Tokenizer(num_words=top_words, filters='', oov_token=True)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    X_train = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    print(\"{}\\n{}\\n\".format(X_train[0:5], X_test[0:5]))\n",
    "\n",
    "    # Convert to sequences\n",
    "    X_train = sequence.pad_sequences(X_train, maxlen=title_length)\n",
    "    X_test = sequence.pad_sequences(X_test, maxlen=title_length)\n",
    "    \n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n",
    "    \n",
    "    run.log_list(\"training_acc\", history.history['accuracy'])\n",
    "    run.log_list(\"validation_acc\", history.history['val_accuracy'])\n",
    "    run.log_list(\"training_loss\", history.history['loss'])\n",
    "    run.log_list(\"validation_loss\", history.history['val_loss'])\n",
    "    \n",
    "    if (not os.path.exists(\"models\")): os.mkdir(\"models/\")\n",
    "    \n",
    "    model_name = \"models/model_topwords_\" + str(top_words)\n",
    "    model.save(model_name)\n",
    "    \n",
    "    run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set()\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "epochs = range(1, len(acc)+1)\n",
    "\n",
    "plt.plot(epochs, acc, '-', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, ':', label='Validation accuracy')\n",
    "plt.title(\"Accuracy curves\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.plot(epochs, loss, '-', label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, '-', label=\"Validation loss\")\n",
    "plt.title(\"Loss curves\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_line(text, tk):\n",
    "    text = process(text)\n",
    "    text = tokenizer.texts_to_sequences([text])\n",
    "    print(\"Tokenised text: \", text)\n",
    "    text = sequence.pad_sequences(text, maxlen=title_length)\n",
    "    return text\n",
    "\n",
    "def predict_input(text):\n",
    "    prediction = model.predict(preprocess_line(text, tokenizer))\n",
    "    output = \"Yes\" if (prediction > 0.5) else \"No\"\n",
    "    print(\"Likelihood of title clickbait: {}%\".format(prediction*100))\n",
    "    print(\"Is it clickbait? {}\".format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = input(\"Enter a title: \")\n",
    "predict_input(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
